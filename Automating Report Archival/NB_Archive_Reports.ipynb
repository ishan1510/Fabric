{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "892d5dd7-fb45-4cef-adfc-7752b9700c3c",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea6daa8-9bee-452e-b8a4-914c9051e98e",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "import sempy\n",
    "import sempy.fabric as fabric\n",
    "import sempy_labs as labs\n",
    "\n",
    "from pyspark.sql.functions import col, count\n",
    "import pandas as pd\n",
    "\n",
    "from sempy_labs import admin, graph\n",
    "from sempy_labs.tom import connect_semantic_model\n",
    "\n",
    "from datetime import datetime\n",
    "import msal, requests\n",
    "\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.keyvault.secrets import SecretClient\n",
    "\n",
    "import requests, time, pandas as pd, concurrent.futures as cf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a667cc-6fe1-478b-a91a-9a3be18228fb",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1c350b-7a32-4cdc-b08c-b6e41bd22382",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "archive_workspace = 'your-archive-workspace-id'\n",
    "\n",
    "KEY_VAULT_URI =\"your-keyvault-url\"\n",
    "TENANT_ID_SECRET = \"TenantID\"\n",
    "CLIENT_ID_SECRET  = \"ClientID\"\n",
    "CLIENT_SECRET_SECRET = \"ClientSecret\"\n",
    "\n",
    "SCOPE = [\"https://api.fabric.microsoft.com/.default\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b642e23-fa0b-487a-9d35-5b34d7fa8695",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### Get secrets from Keyvault"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8ac9e7-6831-4a93-ad16-6ce0c5da8694",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "class _FabricKVToken:\n",
    "    def get_token(self, *scopes, **kwargs):\n",
    "        t = mssparkutils.credentials.getToken(\"https://vault.azure.net\")\n",
    "        return type(\"Tok\", (), {\"token\": t, \"expires_on\": int(time.time()) + 3600})()\n",
    "\n",
    "# ---------  Read SP credentials from Key Vault ----------\n",
    "kv = SecretClient(vault_url=KEY_VAULT_URI, credential=_FabricKVToken())\n",
    "TENANT_ID    = kv.get_secret(TENANT_ID_SECRET).value\n",
    "CLIENT_ID    = kv.get_secret(CLIENT_ID_SECRET).value\n",
    "CLIENT_SECRET = kv.get_secret(CLIENT_SECRET_SECRET).value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98165e9-97e8-4dc1-89b8-b40c9477aa7e",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### Get Unused Reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4c4818-12bc-4f80-add7-22ff00fd694d",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "reports = spark.sql(\"\"\"\n",
    "SELECT r.`Report Id`\n",
    "FROM LH_Monitoring.dbo.dim_reports r\n",
    "LEFT JOIN LH_Monitoring.dbo.dim_workspaces w \n",
    "    ON r.`Workspace Id` = w.Id\n",
    "LEFT JOIN LH_Monitoring.dbo.fact_audit_logs l \n",
    "    ON r.`Report Id` = l.`Report Id`\n",
    "   AND l.Operation = 'ViewReport'\n",
    "   AND l.`Creation Time` > date_sub(current_timestamp(), 90)\n",
    "WHERE w.Type = 'Workspace'\n",
    "  AND r.`Created Date` < date_sub(current_timestamp(), 90)\n",
    "  AND r.IsDeleted = 0\n",
    "GROUP BY r.`Report Id`\n",
    "HAVING COUNT(l.Id) = 0\n",
    "\"\"\")\n",
    "\n",
    "report_list = reports.rdd.flatMap(lambda x: x).collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8443d2bc-e12f-41be-9129-000c4fa488c9",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### Copy Unused Reports to Archive Workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97fa64b-aab9-496b-8672-db7aeef237cd",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "for r in report_list:\n",
    "\n",
    "    # Set required variables\n",
    "    report_name = spark.sql(f\"SELECT `Report Name` FROM LH_Monitoring.dbo.dim_reports WHERE `Report Id` = '{r}'\").collect()[0][0]\n",
    "    date_suffix = datetime.now().strftime(\"%d%m%Y%H%M%S\")\n",
    "    destination_report_name = report_name+\"-\"+date_suffix\n",
    "    report_workspace = spark.sql(f\"SELECT `Workspace Id` FROM LH_Monitoring.dbo.dim_reports WHERE `Report Id` = '{r}'\").collect()[0][0]\n",
    "\n",
    "    # Copy report\n",
    "    df = labs.copy_item(item= r\n",
    "    , type= 'Report'\n",
    "    , source_workspace= report_workspace\n",
    "    , target_name= destination_report_name\n",
    "    ,target_workspace= archive_workspace\n",
    "    ,overwrite=True\n",
    "    ,keep_existing_bindings= True\n",
    "    )\n",
    "\n",
    "    # Insert record into Report Archive History Table\n",
    "    spark.sql(f\"INSERT INTO report_archive_history VALUES ('{r}', '{report_workspace}', now(), '{report_name}', '{destination_report_name}',NULL, NULL)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df195bc-c168-4ce7-b55d-dc0fc17e3f0f",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### Delete Report from Source Workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54cde349-b01d-4593-bc3e-db9b3d2b5935",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "for r in report_list:\n",
    "    REPORT_ID     = r   # the report to delete\n",
    "    WORKSPACE_ID  = spark.sql(f\"SELECT `Workspace Id` FROM LH_Monitoring.dbo.dim_reports WHERE `Report Id` = '{r}'\").collect()[0][0]\n",
    "\n",
    "    # Get token for Fabric REST\n",
    "    authority = f\"https://login.microsoftonline.com/{TENANT_ID}\"\n",
    "    app = msal.ConfidentialClientApplication(CLIENT_ID, authority=authority, client_credential=CLIENT_SECRET)\n",
    "    token = app.acquire_token_for_client(scopes=SCOPE)\n",
    "    access_token = token[\"access_token\"]\n",
    "\n",
    "    # Call Delete Report\n",
    "    url = f\"https://api.fabric.microsoft.com/v1/workspaces/{WORKSPACE_ID}/reports/{REPORT_ID}\"\n",
    "    resp = requests.delete(url, headers={\"Authorization\": f\"Bearer {access_token}\"})\n",
    "    print(resp.status_code, resp.text or \"Deleted (200)\")"
   ]
  }
 ],
 "metadata": {
  "dependencies": {
   "environment": {
    "environmentId": "04d55a02-83bd-4b51-aab1-95515a3a9ffa",
    "workspaceId": "9c9cf6ef-dc0e-4c69-8082-03cb9c63f69c"
   },
   "lakehouse": {
    "default_lakehouse": "c69e199d-9923-4b2e-8523-b0d8b4b7c08d",
    "default_lakehouse_name": "LH_Monitoring",
    "default_lakehouse_workspace_id": "9c9cf6ef-dc0e-4c69-8082-03cb9c63f69c",
    "known_lakehouses": [
     {
      "id": "c69e199d-9923-4b2e-8523-b0d8b4b7c08d"
     }
    ]
   }
  },
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "synapse_pyspark",
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "language_group": "synapse_pyspark",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
