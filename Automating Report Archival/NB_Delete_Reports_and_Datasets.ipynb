{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d85255d-434f-4902-a078-7a1e26ba361d",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531f85ad-0636-41c8-86fd-c7e1d535f87b",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from azure.identity import ClientSecretCredential\n",
    "\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.keyvault.secrets import SecretClient\n",
    "\n",
    "import requests, time, pandas as pd, concurrent.futures as cf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e61186e-fc30-4c15-b511-3df92942fd19",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### Define variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b379cf1e-752e-4e1e-881d-dc39e44c91ce",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "KEY_VAULT_URI =\"your-keyvault-url\"\n",
    "TENANT_ID_SECRET = \"TenantID\"\n",
    "CLIENT_ID_SECRET  = \"ClientID\"\n",
    "CLIENT_SECRET_SECRET = \"ClientSecret\"\n",
    "\n",
    "ARCHIVE_WORKSPACE_ID  = \"YOUR_ARCHIVE_WORKSPACE_ID\"  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6255c49a-8af9-4d78-af95-9bd4cd6030c5",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### Get SP credentails from Key Vault"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160032dc-0b6f-4eb2-a0f5-cd2a5b68c825",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "class _FabricKVToken:\n",
    "    def get_token(self, *scopes, **kwargs):\n",
    "        t = mssparkutils.credentials.getToken(\"https://vault.azure.net\")\n",
    "        return type(\"Tok\", (), {\"token\": t, \"expires_on\": int(time.time()) + 3600})()\n",
    "\n",
    "# ---------  Read SP credentials from Key Vault ----------\n",
    "kv = SecretClient(vault_url=KEY_VAULT_URI, credential=_FabricKVToken())\n",
    "TENANT_ID    = kv.get_secret(TENANT_ID_SECRET).value\n",
    "CLIENT_ID    = kv.get_secret(CLIENT_ID_SECRET).value\n",
    "CLIENT_SECRET = kv.get_secret(CLIENT_SECRET_SECRET).value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0517e3-3c90-4203-8fcb-5a21421ce454",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### Delete Archived Reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ded270-fff9-4245-9524-4dab96ff6f90",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "df_reports = spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        `Dataset Id`\n",
    "    FROM LH_Monitoring.dbo.dim_reports\n",
    "    WHERE `Workspace Id` = '{ARCHIVE_WORKSPACE_ID}'\n",
    "      AND IsDeleted = 0\n",
    "      AND `Created Date` < date_sub(current_timestamp(), 30)\n",
    "\"\"\")\n",
    "\n",
    "report_list = df_reports.rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "for r in report_list:\n",
    "    REPORT_ID     = r   # the report to delete\n",
    "\n",
    "    # Get token for Fabric REST\n",
    "    authority = f\"https://login.microsoftonline.com/{TENANT_ID}\"\n",
    "    app = msal.ConfidentialClientApplication(CLIENT_ID, authority=authority, client_credential=CLIENT_SECRET)\n",
    "    token = app.acquire_token_for_client(scopes=SCOPE)\n",
    "    access_token = token[\"access_token\"]\n",
    "\n",
    "    # Call Delete Report\n",
    "    url = f\"https://api.fabric.microsoft.com/v1/workspaces/{ARCHIVE_WORKSPACE_ID}/reports/{REPORT_ID}\"\n",
    "    resp = requests.delete(url, headers={\"Authorization\": f\"Bearer {access_token}\"})\n",
    "    print(resp.status_code, resp.text or \"Deleted (200)\")\n",
    "\n",
    "    spark.sql(f\"UPDATE report_archive_history SET DeletedOn = now() WHERE ReportId = '{REPORT_ID}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a501b5-9bae-48a2-9188-a683ee328329",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### Delete Unused Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080ce168-a5eb-4c3c-958f-3b6fd230b08b",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# Get Token\n",
    "scope = \"https://analysis.windows.net/powerbi/api/.default\"\n",
    "cred  = ClientSecretCredential(tenant_id=TENANT_ID, client_id=CLIENT_ID, client_secret=CLIENT_SECRET)\n",
    "token = cred.get_token(scope).token\n",
    "headers = {\"Authorization\": f\"Bearer {token}\"}\n",
    "\n",
    "df_datasets = spark.sql(f\"\"\"\n",
    "SELECT \n",
    "    d.`Dataset Id`, \n",
    "    COUNT(r.`Report Id`) AS Report_Count\n",
    "FROM dim_datasets d\n",
    "LEFT JOIN dim_reports r \n",
    "    ON d.`Dataset Id` = r.`Dataset Id`\n",
    "    AND r.IsDeleted = 0\n",
    "    AND  r.`Workspace Id` =! '{ARCHIVE_WORKSPACE_ID}'\n",
    "WHERE d.`Dataset Id` IN (\n",
    "    SELECT \n",
    "        `Dataset Id`\n",
    "    FROM LH_Monitoring.dbo.dim_reports\n",
    "    WHERE `Workspace Id` = '{ARCHIVE_WORKSPACE_ID}'\n",
    "      AND IsDeleted = 0\n",
    "      AND `Created Date` < date_sub(current_timestamp(), 5)\n",
    ")\n",
    "\n",
    "GROUP BY \n",
    "    d.`Dataset Id`\n",
    "HAVING \n",
    "    COUNT(r.`Report Id`) < 1;\n",
    "\n",
    "\"\"\")\n",
    "\n",
    "dataset_list = df_datasets.rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "for DATASET_ID in dataset_list:\n",
    "\n",
    "    WORKSPACE_ID = spark.sql(f\"SELECT `Workspace Id` FROM dim_datasets WHERE `Dataset Id` = '{DATASET_ID}'\").collect()[0][0]\n",
    "    # ==== Delete dataset (workspace-scoped endpoint is safest) ====\n",
    "    url = f\"https://api.powerbi.com/v1.0/myorg/groups/{WORKSPACE_ID}/datasets/{DATASET_ID}\"\n",
    "    r = requests.delete(url, headers=headers)\n",
    "\n",
    "    if r.status_code in (200, 202, 204):\n",
    "        print(f\"✔ Deleted dataset {DATASET_ID} in workspace {WORKSPACE_ID}\")\n",
    "    else:\n",
    "        print(f\"✖ Delete failed: {r.status_code} {r.text[:300]}\")"
   ]
  }
 ],
 "metadata": {
  "dependencies": {
   "environment": {
    "environmentId": "04d55a02-83bd-4b51-aab1-95515a3a9ffa",
    "workspaceId": "9c9cf6ef-dc0e-4c69-8082-03cb9c63f69c"
   },
   "lakehouse": {
    "default_lakehouse": "c69e199d-9923-4b2e-8523-b0d8b4b7c08d",
    "default_lakehouse_name": "LH_Monitoring",
    "default_lakehouse_workspace_id": "9c9cf6ef-dc0e-4c69-8082-03cb9c63f69c",
    "known_lakehouses": [
     {
      "id": "c69e199d-9923-4b2e-8523-b0d8b4b7c08d"
     }
    ]
   }
  },
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "synapse_pyspark",
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "language_group": "synapse_pyspark",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
