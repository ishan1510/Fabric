{"cells":[{"cell_type":"markdown","source":["#### Import Libraries and set Spark Config"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"ee257915-d7d9-465e-b9e2-c4e1213249bf"},{"cell_type":"code","source":["import sempy\n","import sempy.fabric as fabric\n","import sempy_labs as labs\n","from pyspark.sql.functions import col, count\n","from datetime import datetime, timedelta"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"29f03bbf-7a7c-4d53-a5f0-8e8c2bebb9f4"},{"cell_type":"code","source":["spark.conf.set(\"spark.databricks.delta.properties.defaults.minWriterVersion\", 5)\n","spark.conf.set(\"spark.databricks.delta.properties.defaults.minReaderVersion\", 2)"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"df4be96b-41b2-41f4-b183-ff7be981fc32"},{"cell_type":"markdown","source":["#### Fetch Activity Log Data & Save it a Delta Table\n","(Comment or remove below cell when you will schedule this notebook to run daily)"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"9a641ba6-0f2c-4241-ab8b-94f3b63e8db9"},{"cell_type":"code","source":["log_date = '2025-08-15'\n","\n","df_audit_logs = labs.admin.list_activity_events(start_time=f'{log_date}T00:00:00', end_time=f'{log_date}T23:59:59')\n","display(df_audit_logs)\n","\n","spark_df_audit_logs = spark.createDataFrame(df_audit_logs)\n","spark_df_audit_logs.write.mode(\"overwrite\").format(\"delta\").option(\"delta.columnMapping.mode\", \"name\").saveAsTable(\"fact_audit_log_v2\")\n"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"4eca2ea1-f8c6-4b30-a94d-a638e6163beb"},{"cell_type":"markdown","source":["#### Get logs for last 25 days \n","(Comment or remove below cell when you will schedule this notebook to run daily)"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"4f14e8f3-0d0b-46c8-ba06-e81b8ba700b0"},{"cell_type":"code","source":["# Today's date\n","today = datetime.today()\n","\n","start_date = today - timedelta(days=25)\n","\n","# Loop through the last 25 days\n","for i in range(25):\n","    date = start_date + timedelta(days=i)\n","    formated_date = date.strftime('%Y-%m-%d')\n","    print(formated_date)\n","    df_audit_logs = labs.admin.list_activity_events(start_time=f'{formated_date}T00:00:00', end_time=f'{formated_date}T23:59:59')\n","\n","    if df_audit_logs.shape[0] == 0:\n","        print(\"nothing to write\")\n","\n","    else:\n","        # Convert to Spark DataFrame\n","        spark_df_audit_logs = spark.createDataFrame(df_audit_logs)\n","        spark_df_audit_logs.write.format(\"Delta\").mode(\"append\").save(\"your-tables-abfs-path\")\n","        print(\"Data written\")\n","\n"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"02f18686-970d-4274-ba2e-10d61a249f7d"},{"cell_type":"markdown","source":["#### Append Yesterday's Data (For Daily Incremental Load)"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"acbb808c-9779-4f56-9cda-944de8e878e7"},{"cell_type":"code","source":["yesterday = datetime.today() - timedelta(days=1)\n","\n","formated_date = yesterday.strftime('%Y-%m-%d')\n","\n","print(formated_date)\n","\n","df_audit_logs = labs.admin.list_activity_events(start_time=f'{formated_date}T00:00:00', end_time=f'{formated_date}T23:59:59')\n","\n","if df_audit_logs.shape[0] == 0:\n","    print(\"nothing to write\")\n","\n","else:\n","    # Convert to Spark DataFrame\n","    spark_df_audit_logs = spark.createDataFrame(df_audit_logs)\n","    spark_df_audit_logs.write.format(\"Delta\").mode(\"append\").save(\"your-tables-abfs-path\")"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"b8e6c53c-125d-45b1-ba77-383d201c0e9c"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","display_name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"synapse_widget":{"version":"0.1","state":{}},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"default_lakehouse":"c69e199d-9923-4b2e-8523-b0d8b4b7c08d","known_lakehouses":[{"id":"c69e199d-9923-4b2e-8523-b0d8b4b7c08d"}],"default_lakehouse_name":"LH_Monitoring","default_lakehouse_workspace_id":"9c9cf6ef-dc0e-4c69-8082-03cb9c63f69c"},"environment":{"environmentId":"04d55a02-83bd-4b51-aab1-95515a3a9ffa","workspaceId":"9c9cf6ef-dc0e-4c69-8082-03cb9c63f69c"}}},"nbformat":4,"nbformat_minor":5}